{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL-2 (FFNN)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "len(x_train)\n",
    "len(x_test) \n",
    "len(y_test)\n",
    "len(y_train)\n",
    "x_train.shape\n",
    "x_train[0]\n",
    "plt.matshow(x_train[0])\n",
    "plt.imshow(-x_train[0], cmap=\"gray\")\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "x_train[0]\n",
    "\n",
    "model = keras.Sequential([\n",
    "keras.layers.Flatten(input_shape=(28, 28)),    #Input layer\n",
    "keras.layers.Dense(128, activation=\"relu\"),    #hidden layer abs\n",
    "keras.layers.Dense(10, activation=\"softmax\")   #output layer\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"sgd\",   # Stochastic Gradient Descent\n",
    "loss=\"sparse_categorical_crossentropy\",    # crossentropy reduces the loss\n",
    "metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10)\n",
    "\n",
    "test_loss,test_acc=model.evaluate(x_test,y_test)\n",
    "print(\"Loss=%.3f\" %test_loss)\n",
    "print(\"Accuracy=%.3f\" %test_acc)\n",
    "\n",
    "n=random.randint(0,9999)\n",
    "plt.imshow(x_test[n])\n",
    "plt.show()\n",
    "\n",
    "#we use predict() on new data\n",
    "predicted_value=model.predict(x_test)\n",
    "print(\"Handwritten number in the image is= %d\" %np.argmax(predicted_value[n]))\n",
    "\n",
    "history.history??\n",
    "history.history.keys()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Accuracy and loss on tarin data\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training Loss and accuracy')\n",
    "plt.ylabel('accuracy/Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'val_accuracy','loss','val_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL-3 (CNN)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Conv2D,Dense,MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train[0].min(),X_train[0].max()\n",
    "\n",
    "X_train = (X_train - 0.0) / (255.0 - 0.0)\n",
    "X_test = (X_test - 0.0) / (255.0 - 0.0)\n",
    "X_train[0].min(), X_train[0].max()\n",
    "(0.0, 1.0)\n",
    "\n",
    "def plot_digit(image, digit, plt, i):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "    plt.title(f\"Digit: {digit}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i in range(20):\n",
    "    plot_digit(X_train[i], y_train[i], plt, i)\n",
    "plt.show()\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape+ (1,)))\n",
    "X_test = X_test.reshape((X_test.shape+(1,)))\n",
    "\n",
    "y_train[0:20]\n",
    "\n",
    "model = Sequential([                                     # model create \n",
    "    Conv2D(32,(3,3), activation=\"relu\",input_shape=(28,28,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(100,activation=\"relu\"),\n",
    "    Dense(10,activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "Model_log=model.fit(X_train, y_train, epochs=10, batch_size=15, verbose=1, validation_data=(X_test, y_test));\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i in range(20):\n",
    "    image = random.choice(X_test).squeeze()\n",
    "    digit = np.argmax(model.predict(image.reshape((1, 28, 28, 1)))[0], axis=-1)\n",
    "    plot_digit(image, digit, plt, i)\n",
    "plt.show()\n",
    "\n",
    "predictions = np.argmax(model.predict(X_test),axis=-1)\n",
    "accuracy_score(y_test,predictions)\n",
    "\n",
    "n = random.randint(0,9999)\n",
    "plt.imshow(X_test[n])\n",
    "plt.show()\n",
    "\n",
    "predicted_value = model.predict(X_test)\n",
    "print(\"Handwritten number in the image is = %d\" %np.argmax(predicted_value[n]))\n",
    "\n",
    "score = model.evaluate(X_test,y_test,verbose=0)\n",
    "print('Test loss:' , score[0])\n",
    "print('Testaccuracy:',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL-6 (VGG16)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "train_dir = \"C:/Users/sanka/OneDrive/Desktop/Data sets/cifar-10-img/cifar-10-img/train\"\n",
    "test_dir = \"C:/Users/sanka/OneDrive/Desktop/Data sets/cifar-10-img/cifar-10-img/test\"\n",
    "\n",
    "train_datagen = ImageDataGenerator( rescale=1.0 / 255,)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255, )\n",
    "\n",
    "# here batch_size is the number of images in each batch\n",
    "train_batch_size = 5000\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(32, 32),\n",
    "    batch_size=train_batch_size,\n",
    "    class_mode='categorical'   )\n",
    "test_batch_size = 1000\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(32, 32),\n",
    "    batch_size=test_batch_size,\n",
    "    class_mode='categorical'  )\n",
    "\n",
    "x_train, y_train =  train_generator[0]\n",
    "x_test, y_test = test_generator[0]\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "\n",
    "# Load VGG16 without top layers\n",
    "weights_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "   layer.trainable = False\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(32, 32, 3))\n",
    "# freeze all layers first\n",
    "for layer in base_model.layers:\n",
    "   layer.trainable = False\n",
    "# unfreeze last 4 layers of base model\n",
    "for layer in base_model.layers[len(base_model.layers) - 4:]:\n",
    "   layer.trainable = True\n",
    "# fine-tuning hyper parameters\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# training fine tuned model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "predicted_value = model.predict(x_test)\n",
    "\n",
    "labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "n=945\n",
    "plt.imshow(x_test[n])\n",
    "print(\"Preditcted: \",labels[np.argmax(predicted_value[n])])\n",
    "print(\"Actual: \", labels[np.argmax(y_test[n])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL-4(Autoencoder)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n",
    "\n",
    "RANDOM_SEED = 2021\n",
    "TEST_PCT = 0.3\n",
    "LABELS = [\"Normal\",\"Fraud\"]\n",
    "\n",
    "dataset = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "#check for any null values\n",
    "print(\"Any nulls in the dataset\",dataset.isnull().values.any())\n",
    "print('-------')\n",
    "print(\"No. of unique labels\",len(dataset['Class'].unique()))\n",
    "print(\"Label values\",dataset.Class.unique())\n",
    "#0 is for normal credit card transcation\n",
    "#1 is for fraudulent credit card transcation\n",
    "print('-------')\n",
    "print(\"Break down of Normal and Fraud Transcations\")\n",
    "print(pd.value_counts(dataset['Class'],sort=True))\n",
    "\n",
    "#visualizing the imbalanced dataset\n",
    "count_classes = pd.value_counts(dataset['Class'],sort=True)\n",
    "count_classes.plot(kind='bar',rot=0)\n",
    "plt.xticks(range(len(dataset['Class'].unique())),dataset.Class.unique())\n",
    "plt.title(\"Frequency by observation number\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Observations\")\n",
    "normal_dataset = dataset[dataset.Class == 0]\n",
    "fraud_dataset = dataset[dataset.Class == 1]\n",
    "\n",
    "#Save the normal and fradulent transcations in seperate dataframe\n",
    "normal_dataset = dataset[dataset.Class == 0]\n",
    "fraud_dataset = dataset[dataset.Class == 1]\n",
    "#Visualize transcation amounts for normal and fraudulent transcations\n",
    "bins = np.linspace(200,2500,100)\n",
    "plt.hist(normal_dataset.Amount,bins=bins,alpha=1,density=True,label='Normal')\n",
    "plt.hist(fraud_dataset.Amount,bins=bins,alpha=0.5,density=True,label='Fraud')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Transcation Amount vs Percentage of Transcaions\")\n",
    "plt.xlabel(\"Transcation Amount (USD)\")\n",
    "plt.ylabel(\"Percentage of Transcations\")\n",
    "plt.show()\n",
    "\n",
    "dataset\n",
    "\n",
    "sc = StandardScaler()\n",
    "dataset['Time'] = sc.fit_transform(dataset['Time'].values.reshape(-1,1))\n",
    "dataset['Amount'] = sc.fit_transform(dataset['Amount'].values.reshape(-1,1))\n",
    "\n",
    "raw_data = dataset.values\n",
    "#The last element contains if the transcation is normal which is represented by 0 and if fraud then 1\n",
    "labels = raw_data[:,-1]\n",
    "#The other data points are the electrocadriogram data\n",
    "data = raw_data[:,0:-1]\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size = 0.2,random_state =2021)\n",
    "\n",
    "min_val = tf.reduce_min(train_data)\n",
    "max_val = tf.reduce_max(train_data)\n",
    "train_data = (train_data - min_val) / (max_val - min_val)\n",
    "test_data = (test_data - min_val) / (max_val - min_val)\n",
    "train_data = tf.cast(train_data,tf.float32)\n",
    "test_data = tf.cast(test_data,tf.float32)\n",
    "\n",
    "train_labels = train_labels.astype(bool)\n",
    "test_labels = test_labels.astype(bool)\n",
    "#Creating normal and fraud datasets\n",
    "normal_train_data = train_data[~train_labels]\n",
    "normal_test_data = test_data[~test_labels]\n",
    "fraud_train_data = train_data[train_labels]\n",
    "fraud_test_data = test_data[test_labels]\n",
    "print(\"No. of records in Fraud Train Data=\",len(fraud_train_data))\n",
    "print(\"No. of records in Normal Train Data=\",len(normal_train_data))\n",
    "print(\"No. of records in Fraud Test Data=\",len(fraud_test_data))\n",
    "print(\"No. of records in Normal Test Data=\",len(normal_test_data))\n",
    "\n",
    "nb_epoch = 50\n",
    "batch_size = 64\n",
    "input_dim = normal_train_data.shape[1]\n",
    "#num of columns,30\n",
    "encoding_dim = 14\n",
    "hidden_dim1 = int(encoding_dim / 2)\n",
    "hidden_dim2 = 4\n",
    "learning_rate = 1e-7\n",
    "\n",
    "#input layer\n",
    "# Build models\n",
    "input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "#Encoder\n",
    "encoder = tf.keras.layers.Dense(encoding_dim,activation=\"tanh\",activity_regularizer = tf.keras.regularizers.l2(learning_rate))(input_layer)\n",
    "encoder = tf.keras.layers.Dropout(0.2)(encoder)\n",
    "encoder = tf.keras.layers.Dense(hidden_dim1,activation='relu')(encoder)\n",
    "encoder = tf.keras.layers.Dense(hidden_dim2,activation=tf.nn.leaky_relu)(encoder)\n",
    "#Decoder\n",
    "decoder = tf.keras.layers.Dense(hidden_dim1,activation='relu')(encoder)\n",
    "decoder = tf.keras.layers.Dropout(0.2)(decoder)\n",
    "decoder = tf.keras.layers.Dense(encoding_dim,activation='relu')(decoder)\n",
    "decoder = tf.keras.layers.Dense(input_dim,activation='tanh')(decoder)\n",
    "#Autoencoder\n",
    "# combine encoder and decoder to create the autoencoder model\n",
    "autoencoder = tf.keras.Model(inputs = input_layer,outputs = decoder)\n",
    "autoencoder.summary()\n",
    "\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(filepath=\"autoencoder_fraud.h5\",mode='min',monitor='val_loss',verbose=2,save_best_only=True)\n",
    "#Define our early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=0.0001,\n",
    "   patience=10, verbose=11, mode='min', restore_best_weights=True )\n",
    "\n",
    "autoencoder.compile(metrics=['accuracy'],loss= 'mean_squared_error',optimizer='adam')\n",
    "\n",
    "history = autoencoder.fit(normal_train_data,normal_train_data,epochs = nb_epoch,\n",
    "                         batch_size = batch_size,shuffle = True,\n",
    "                         validation_data = (test_data,test_data),\n",
    "                         verbose=1,\n",
    "                         callbacks = [cp,early_stop]).history\n",
    "\n",
    "plt.plot(history['loss'],linewidth = 2,label = 'Train')\n",
    "plt.plot(history['val_loss'],linewidth = 2,label = 'Test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "#plt.ylim(ymin=0.70,ymax=1)\n",
    "plt.show()\n",
    "\n",
    "test_x_predictions = autoencoder.predict(test_data)\n",
    "mse = np.mean(np.power(test_data - test_x_predictions, 2),axis = 1)\n",
    "error_df = pd.DataFrame({'Reconstruction_error':mse,\n",
    "                         'True_class':test_labels})\n",
    "\n",
    "threshold_fixed = 50\n",
    "groups = error_df.groupby('True_class')\n",
    "fig,ax = plt.subplots()\n",
    "for name,group in groups:\n",
    "        ax.plot(group.index,group.Reconstruction_error,marker='o',ms = 3.5,linestyle='',\n",
    "                label = \"Fraud\" if  name==1 else \"Normal\")\n",
    "ax.hlines(threshold_fixed,ax.get_xlim()[0],ax.get_xlim()[1],colors=\"r\",zorder=100,label=\"Threshold\")\n",
    "ax.legend()\n",
    "plt.title(\"Reconstructions error for normal and fraud data\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show()\n",
    "\n",
    "threshold_fixed = 52\n",
    "pred_y = [1 if e > threshold_fixed else 0 \n",
    "          for e in \n",
    "        error_df.Reconstruction_error.values]\n",
    "error_df['pred'] = pred_y\n",
    "conf_matrix = confusion_matrix(error_df.True_class,pred_y)\n",
    "\n",
    "plt.figure(figsize = (4,4))\n",
    "sns.heatmap(conf_matrix,xticklabels = LABELS,yticklabels = LABELS,annot = True,fmt=\"d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.show()\n",
    "\n",
    "#Print Accuracy,Precision and Recall\n",
    "print(\"Accuracy :\",accuracy_score(error_df['True_class'],error_df['pred']))\n",
    "print(\"Recall :\",recall_score(error_df['True_class'],error_df['pred']))\n",
    "print(\"Precision :\",precision_score(error_df['True_class'],error_df['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL-5 (CBOW)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "sentences = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\"\n",
    "\n",
    "# remove special characters\n",
    "sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
    "# remove 1 letter words\n",
    "sentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n",
    "# lower all characters\n",
    "sentences = sentences.lower()\n",
    "\n",
    "words = sentences.split()      #vocabulary\n",
    "vocab = set(words)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 10\n",
    "context_size = 2\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "embeddings =  np.random.random_sample((vocab_size, embed_dim))\n",
    "\n",
    "def linear(m, theta):\n",
    "    w = theta\n",
    "    return m.dot(w)\n",
    "\n",
    "def log_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.log(e_x / e_x.sum())\n",
    "\n",
    "def NLLLoss(logs, targets):\n",
    "    out = logs[range(len(targets)), targets]\n",
    "    return -out.sum()/len(out)\n",
    "\n",
    "def log_softmax_crossentropy_with_logits(logits,target):\n",
    "    out = np.zeros_like(logits)\n",
    "    out[np.arange(len(logits)),target] = 1\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    return (- out + softmax) / logits.shape[0]\n",
    "\n",
    "def forward(context_idxs, theta):\n",
    "    m = embeddings[context_idxs].reshape(1, -1)\n",
    "    n = linear(m, theta)\n",
    "    o = log_softmax(n)\n",
    "    return m, n, o\n",
    "\n",
    "def backward(preds, theta, target_idxs):\n",
    "    m, n, o = preds\n",
    "    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n",
    "    dw = m.T.dot(dlog)\n",
    "    return dw\n",
    "\n",
    "def optimize(theta, grad, lr=0.03):\n",
    "    theta -= grad * lr\n",
    "    return theta\n",
    "\n",
    "theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))\n",
    "\n",
    "epoch_losses = {}\n",
    "for epoch in range(80):\n",
    "    losses =  []\n",
    "    for context, target in data:\n",
    "        context_idxs = np.array([word_to_ix[w] for w in context])\n",
    "        preds = forward(context_idxs, theta)\n",
    "        target_idxs = np.array([word_to_ix[target]])\n",
    "        loss = NLLLoss(preds[-1], target_idxs)\n",
    "        losses.append(loss)\n",
    "        grad = backward(preds, theta, target_idxs)\n",
    "        theta = optimize(theta, grad, lr=0.03)\n",
    "    epoch_losses[epoch] = losses\n",
    "    \n",
    "ix = np.arange(0,80)\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Epoch/Losses', fontsize=20)\n",
    "plt.plot(ix,[epoch_losses[i][0] for i in ix])\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Losses', fontsize=12)\n",
    "\n",
    "def predict(words):\n",
    "    context_idxs = np.array([word_to_ix[w] for w in words])\n",
    "    preds = forward(context_idxs, theta)\n",
    "    word = ix_to_word[np.argmax(preds[-1])]\n",
    "    return word\n",
    "\n",
    "predict(['we', 'are', 'to', 'study'])\n",
    "\n",
    "def accuracy():\n",
    "    wrong = 0\n",
    "    for context, target in data:\n",
    "        if(predict(context) != target):\n",
    "            wrong += 1\n",
    "    return (1 - (wrong / len(data)))\n",
    "\n",
    "accuracy()\n",
    "\n",
    "predict(['processes', 'manipulate', 'things', 'study'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
